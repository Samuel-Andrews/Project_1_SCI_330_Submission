{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Project 1 Part 3 Samuel Andrews"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CERS     SS      NEW        2,756   2.400000    2.340000   2.45000006/1406/191706149900003 09:30:CustSS /17  /17  256945       00 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=GSCO\\n',\n",
       " 'CERS     SS      NEW          100   2.36000018422.360000   2.37000006/1406/191706149900003 10:20:ContraSSFREX /17  /17  866087       04 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n']"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#read in the file\n",
    "\n",
    "with open('project_1_part_2_output.txt') as f:\n",
    "    contentlines = f.readlines()\n",
    "contentlines[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "While the data in its current state is now all wrapped up into individual lines, we unfortunatley have a new issue. From the starting document, 17 varibles are listed. However, based on the output there are 22 data items. 2 of those item appear to be the year portion of the two dates listed off (as shown in the top portion of the triage discussion), but the remaining three take the forms \"AABBC\", \"1706149900003#STA=CERS\", and \"854300\" in most cases, with the latter somtimes not being present at all. Due to how the dates ended up being spread across multiple lines, we can infer that some og these trailing items are simply parts of variables in the wrong place, which is best scene when viewing the data in its original form. It seems that every other line represents a \"cut-out\" of the line above it, allows us to infer that the the floating /17's belong on the ends of the msuhed together dates, the standalone 2 digits are the seconds in the \"Hour:Minutes:Seconds\" and that the first set of random digits is likely the tail end of the digits smushed against the data. That leaves two remaining items, one of which is optional.\n",
    "\n",
    "Logically, they don't appear to fit anywhere, and without metadata I have no way to know for sure, so to avoid the risk of unintentionally destorying the data, I will place them in seperate columns with the understanding that in a \"real-world\" case there would be someone I could talk to who would know where which column these items belong two and one could easily pair them up using simple list comprehensions to put in its ture form after gleaning such."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Given what was discussed above and the nature of our overall approach, the first goal will to be to put every line in a similar enough form to pull out the data items with the same logic. Starting off, those random \"/17\" are always the same, so lets remove them outright so they don't have to be dealt with.\n",
    "\n",
    "First, we need a function to replace such transgressions with empty space"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'CERS     SS      NEW          100   2.36000018422.360000   2.37000006/1406/191706149900003 10:20:ContraSSFREX /17  /17  866087       04 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "#get testline\n",
    "\n",
    "testline = contentlines[1]\n",
    "testline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'CERS     SS      NEW          100   2.36000018422.360000   2.37000006/1406/191706149900003 10:20:ContraSSFREX 866087       04 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 179
    }
   ],
   "source": [
    "#test function\n",
    "\n",
    "remove_17 = lambda line: line.replace(\" /17 \", \"\")\n",
    "\n",
    "remove_17(testline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all lines\n",
    "\n",
    "testoupout = [remove_17(line) for line in contentlines ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CERS     SS      NEW        2,756   2.400000    2.340000   2.45000006/1406/191706149900003 09:30:CustSS 256945       00 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=GSCO\\n',\n",
       " 'CERS     SS      NEW          100   2.36000018422.360000   2.37000006/1406/191706149900003 10:20:ContraSSFREX 866087       04 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n',\n",
       " 'CERS     SS      NEW          200   2.350000    2.360000   2.37000006/1406/191706149900003 10:20:ContraSSFREX 866096       04 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n',\n",
       " 'CERS     SS      NEW          100   2.350000    2.350000   2.36000006/1406/191706149900003 10:20:ContraSSFREX 869780       23 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n',\n",
       " 'CERS     SS      NEW          100   2.350000    2.350000   2.36000006/1406/191706149900003 10:22:ContraSSFREX 892888       29 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n',\n",
       " 'CERS     SS      NEW          100   2.360000    2.350000   2.36000006/1406/191706149900003 10:24:ContraSSFREX 909119       01 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS\\n',\n",
       " 'CERS     SS      NEW          100   2.36000085432.350000   2.36000006/1406/191706149900003 10:24:ContraSSFREX 909128       01 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n',\n",
       " 'CERS     SS      NEW          100   2.360000    2.350000   2.36000006/1406/191706149900003 10:24:ContraSSFREX 909140       01 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n',\n",
       " 'CERS     SS      NEW          100   2.360000    2.350000   2.36000006/1406/191706149900003 10:24:ContraSSFREX 909143       01 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n',\n",
       " 'CERS     SS      NEW          100   2.360000    2.350000   2.36000006/1406/191706149900003 10:24:ContraSSFREX 909148       01 0704 PerUnit   TERM  0.010000     AABBC     1706149900003#STA=CERS 854300\\n']"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "#look at it\n",
    "\n",
    "testoupout[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new variable to hold our work as we go through\n",
    "\n",
    "Workingcontent = testoupout"
   ]
  },
  {
   "source": [
    "Next, to keep future regular expressions clean, lets make sure every variable is only seperated by a singular whitespace. A cheeky way of doing this in python is to take advantage of str.split seperating on excess whitespace and then using str.join to rejoin the characters with only a single space."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'CERS SS NEW 100 2.36000018422.360000 2.37000006/1406/191706149900003 10:20:ContraSSFREX /17 /17 866087 04 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300'"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "#split by whiteapce, then recombine seperated by spaces\n",
    "\n",
    "Excess_space_remover = lambda line: \" \".join(line.split())\n",
    "\n",
    "Excess_space_remover(testline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CERS SS NEW 2,756 2.400000 2.340000 2.45000006/1406/191706149900003 09:30:CustSS 256945 00 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=GSCO',\n",
       " 'CERS SS NEW 100 2.36000018422.360000 2.37000006/1406/191706149900003 10:20:ContraSSFREX 866087 04 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 200 2.350000 2.360000 2.37000006/1406/191706149900003 10:20:ContraSSFREX 866096 04 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.350000 2.350000 2.36000006/1406/191706149900003 10:20:ContraSSFREX 869780 23 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.350000 2.350000 2.36000006/1406/191706149900003 10:22:ContraSSFREX 892888 29 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.360000 2.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909119 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS',\n",
       " 'CERS SS NEW 100 2.36000085432.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909128 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.360000 2.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909140 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.360000 2.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909143 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.360000 2.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909148 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300']"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "#do it for all lines\n",
    "\n",
    "testoupout = [Excess_space_remover(line) for line in Workingcontent]\n",
    "\n",
    "testoupout[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn into working content\n",
    "\n",
    "Workingcontent = testoupout"
   ]
  },
  {
   "source": [
    "Because the end result of this project is a clean .csv file, the commas in this file will need to be expunged. Fortunatley,they only ever appear sometimes in the fourth data item, which is presumed to be qty where they cause damage to the numeric data anyway. So while we have the data so neatly spaced, we might as well remove them now similar to how we remove the \"/17s\" a step earlier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'CERS SS NEW 2,756 2.400000 2.340000 2.45000006/1406/191706149900003 09:30:CustSS 256945 00 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=GSCO'"
      ]
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "source": [
    "#showcase the problem\n",
    "\n",
    "testline = testoupout[0]\n",
    "\n",
    "testline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'CERS SS NEW 2756 2.400000 2.340000 2.45000006/1406/191706149900003 09:30:CustSS 256945 00 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=GSCO'"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "source": [
    "#function to fix it\n",
    "\n",
    "remove_comma = lambda line: line.replace(\",\", \"\")\n",
    "\n",
    "remove_comma(testline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CERS SS NEW 2756 2.400000 2.340000 2.45000006/1406/191706149900003 09:30:CustSS 256945 00 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=GSCO',\n",
       " 'CERS SS NEW 100 2.36000018422.360000 2.37000006/1406/191706149900003 10:20:ContraSSFREX 866087 04 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 200 2.350000 2.360000 2.37000006/1406/191706149900003 10:20:ContraSSFREX 866096 04 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.350000 2.350000 2.36000006/1406/191706149900003 10:20:ContraSSFREX 869780 23 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.350000 2.350000 2.36000006/1406/191706149900003 10:22:ContraSSFREX 892888 29 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.360000 2.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909119 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS',\n",
       " 'CERS SS NEW 100 2.36000085432.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909128 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.360000 2.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909140 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.360000 2.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909143 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300',\n",
       " 'CERS SS NEW 100 2.360000 2.350000 2.36000006/1406/191706149900003 10:24:ContraSSFREX 909148 01 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300']"
      ]
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "#for all lines\n",
    "\n",
    "testoupout = [remove_comma(line) for line in Workingcontent ]\n",
    "\n",
    "testoupout[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "Workingcontent = testoupout"
   ]
  },
  {
   "source": [
    "Before we get ahead of ourselves and begin pulling data, we need to talk about a particular error that some lines suffer from. Take line 79 for example and compare it to line 2:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wrong\nCERS SS NEW 79 2.350000 2.350000 2.36000006/1406/191706149900004 10:56:ContraSSFREX 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 271819 34 854300\n\n\n\n\"Correct\"\nCERS SS NEW 100 2.36000018422.360000 2.37000006/1406/191706149900003 10:20:ContraSSFREX 866087 04 0704 PerUnit TERM 0.010000 AABBC 1706149900003#STA=CERS 854300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Wrong\")\n",
    "print(Workingcontent[78])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"\\\"Correct\\\"\")\n",
    "print(Workingcontent[1])\n"
   ]
  },
  {
   "source": [
    "In some cases, we can see that the hanging \"seconds\" item and the miscellanues digit group in front of it are sometimes places just before the end instead of after the time variable. This is not a problem that is insurmountable, but it is very annoying and could lead to some fairly disgusting regex statements. This could be fixed by reorder each line by first determing problem lines via regex and then using indexing to fix it, but because we have yet to fix the variable bleed problem, that solution would be incredibly finicky to pull off. Instead, we will opt to avoid the problem entirely by grabbing off everything up until the discrepancy in order takes place, and than writing the regex statement to deal specifically with that group.\n",
    "\n",
    "So to start, well work item by item until we get to that point to ensure that our pulls are without error until we get to that point. By the end of this list, we should have a bunch of lists sized 913 that we can join together into our clean .csv file.\n",
    "\n",
    "First up, lets grab the symbol using the regex developed for part 2, as we know the ticker."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'CERS'"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "\n",
    "#test regex\n",
    "\n",
    "pullticker = re.compile('^([A-Z][A-Z]?[A-Z]?[A-Z]?)\\s')\n",
    "\n",
    "pullticker.match(Workingcontent[1])[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS',\n",
       " 'CERS']"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "source": [
    "#for all lines\n",
    "\n",
    "\n",
    "Ticker_list = [pullticker.match(line)[1] for line in Workingcontent]\n",
    "\n",
    "Ticker_list[0:20]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "#check length\n",
    "\n",
    "len(Ticker_list)"
   ]
  },
  {
   "source": [
    "The next variable, side, is literally just SS 913 times (as shown by a quick control F, where you get 913*2 times across 2 variables). We could just make a list that size containing only that, but for possbile future use we will build a regex that grabs it all the same."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'SS'"
      ]
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "pullside = re.compile('\\s(SS)\\s')\n",
    "\n",
    "\n",
    "pullside.search(Workingcontent[1])[1] #change to search cause were not in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS',\n",
       " 'SS']"
      ]
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "source": [
    "\n",
    "side_list = [pullside.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "side_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 196
    }
   ],
   "source": [
    "len(side_list)"
   ]
  },
  {
   "source": [
    "Next up is cxl, which is always new across the board just like side and will be treated the same."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'NEW'"
      ]
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "source": [
    "pullcxl = re.compile('\\s(NEW)\\s')\n",
    "\n",
    "\n",
    "pullcxl.search(Workingcontent[1])[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW',\n",
       " 'NEW']"
      ]
     },
     "metadata": {},
     "execution_count": 198
    }
   ],
   "source": [
    "cxl_list = [pullcxl.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "cxl_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "len(cxl_list)"
   ]
  },
  {
   "source": [
    "Next, we have qty, which presents are first real challenge of note. This variable is a number amount seperated by spaces that comes directly after new. It used to contain commas, but those were removed earlier, so we can use new as a base to capture the value off of."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'100'"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "pullqty = re.compile('\\sNEW\\s(\\S*)\\s')\n",
    "\n",
    "\n",
    "pullqty.search(Workingcontent[1])[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['2756',\n",
       " '100',\n",
       " '200',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '100',\n",
       " '300',\n",
       " '200',\n",
       " '100',\n",
       " '300',\n",
       " '100',\n",
       " '100']"
      ]
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "source": [
    "qty_list = [pullqty.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "qty_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "source": [
    "len(qty_list)"
   ]
  },
  {
   "source": [
    "Next, we have price, which somtimes bleeds into the next variable bid, which in turn sometimes bleeds into ask. Ask also bleeds into the next date variable, whihc always starts with a 06 as discussed in Triage. there is also another 06 later in the lines that is also accounted for. It appears the bleeding characters come from an odd rounding error caused by these prices being stored as floats. However, these floats always start with \"x.\", where x is any integer, which always starts each of these numbers off. Homing in on new again, we can use this trick to grab all three in one fell swoop."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.3600001842'"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "pull_price_bid_ask = re.compile('\\sNEW\\s\\S*\\s(\\d\\.\\S*)\\s?(\\d\\.\\S*)\\s?(\\d\\.\\S+)\\s?06\\/\\S+06\\/')\n",
    "\n",
    "pull_price_bid_ask.search(Workingcontent[1])[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "price_list = [pull_price_bid_ask.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "bid_list = [pull_price_bid_ask.search(line)[2] for line in Workingcontent]\n",
    "\n",
    "ask_list = [pull_price_bid_ask.search(line)[3] for line in Workingcontent]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['2.400000', '2.3600001842', '2.350000', '2.350000', '2.350000', '2.360000', '2.3600008543', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.365000', '2.3600008543', '2.360000', '2.360000', '2.360000', '2.360000', '2.365000']\n['2.340000', '2.360000', '2.360000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.350000', '2.360000']\n['2.450000', '2.370000', '2.370000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.360000', '2.370000']\n"
     ]
    }
   ],
   "source": [
    "print(price_list[0:20])\n",
    "\n",
    "print(bid_list[0:20])\n",
    "\n",
    "print(ask_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "913\n913\n913\n"
     ]
    }
   ],
   "source": [
    "print(len(price_list))\n",
    "\n",
    "print(len(bid_list))\n",
    "\n",
    "print(len(ask_list))"
   ]
  },
  {
   "source": [
    "The next two variables, T-Dats-DatTRadeID and TradeTiSS, are always squished together and were already found indirectly by our last solution. We will grab both of them at the same time just the same. Afterwards, we will need to join \"/17\" on the end of each of these list items get them fully cleaned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'06/14'"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "\n",
    "pull_T_DatS_DatTradeID_TradeTiSS = re.compile('\\sNEW\\s\\S*\\s\\d\\.\\S*\\s?\\d\\.\\S*\\s?\\d\\.\\S+\\s?(06\\/\\S+)(06\\/\\d\\d)')\n",
    "\n",
    "\n",
    "pull_T_DatS_DatTradeID_TradeTiSS.search(Workingcontent[1])[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T_DatS_DatTradeID_list = [pull_T_DatS_DatTradeID_TradeTiSS.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "TradeTiSS_list = [pull_T_DatS_DatTradeID_TradeTiSS.search(line)[2] for line in Workingcontent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['06/14', '06/14', '06/14', '06/14', '06/14', '06/14', '06/14', '06/14', '06/14', '06/14']\n['06/19', '06/19', '06/19', '06/19', '06/19', '06/19', '06/19', '06/19', '06/19', '06/19']\n"
     ]
    }
   ],
   "source": [
    "print(T_DatS_DatTradeID_list[0:10])\n",
    "\n",
    "print(TradeTiSS_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "913\n913\n"
     ]
    }
   ],
   "source": [
    "print(len(T_DatS_DatTradeID_list))\n",
    "\n",
    "print(len(TradeTiSS_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['06/14/17', '06/14/17', '06/14/17', '06/14/17', '06/14/17', '06/14/17', '06/14/17', '06/14/17', '06/14/17', '06/14/17']\n['06/19/17', '06/19/17', '06/19/17', '06/19/17', '06/19/17', '06/19/17', '06/19/17', '06/19/17', '06/19/17', '06/19/17']\n"
     ]
    }
   ],
   "source": [
    "#add the \"/17\" back onto the ends of them\n",
    "\n",
    "T_DatS_DatTradeID_list_fixed = [line + \"/17\" for line in T_DatS_DatTradeID_list]\n",
    "\n",
    "TradeTiSS_list_fixed = [line + \"/17\" for line in TradeTiSS_list]\n",
    "\n",
    "print(T_DatS_DatTradeID_list_fixed[0:10])\n",
    "\n",
    "print(TradeTiSS_list_fixed[0:10])"
   ]
  },
  {
   "source": [
    "The next variable, exbr, appears to be some kind of code that always bleeds into the date. However, a 6 digit piece of it appears to have been pulled away from it much like the \"/17\" and the seconds portion of the time. Due to the error desribed earlier, it also is does not consistently appear in the same place, and another optional code might conflict with it. The good news is that it does always come first, so with some regex logic we can still scoop both. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1706149900003\n866087\n"
     ]
    }
   ],
   "source": [
    "pull_exbr = re.compile('\\/\\d\\d(\\d+)\\s.*\\s([0-9]{6})\\s.*\\s([0-9]{6})?\\s?.*?')\n",
    "\n",
    "\n",
    "print(pull_exbr.search(Workingcontent[1])[1])\n",
    "\n",
    "print(pull_exbr.search(Workingcontent[1])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "exbr_part1_list = [pull_exbr.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "exbr_part2_list = [pull_exbr.search(line)[2] for line in Workingcontent]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['1706149900003', '1706149900003', '1706149900003', '1706149900003', '1706149900003', '1706149900003', '1706149900003', '1706149900003', '1706149900003', '1706149900003']\n['256945', '866087', '866096', '869780', '892888', '909119', '909128', '909140', '909143', '909148']\n"
     ]
    }
   ],
   "source": [
    "print(exbr_part1_list[0:10])\n",
    "\n",
    "print(exbr_part2_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "913\n913\n"
     ]
    }
   ],
   "source": [
    "print(len(exbr_part1_list))\n",
    "print(len(exbr_part2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1706149900003256945',\n",
       " '1706149900003866087',\n",
       " '1706149900003866096',\n",
       " '1706149900003869780',\n",
       " '1706149900003892888',\n",
       " '1706149900003909119',\n",
       " '1706149900003909128',\n",
       " '1706149900003909140',\n",
       " '1706149900003909143',\n",
       " '1706149900003909148']"
      ]
     },
     "metadata": {},
     "execution_count": 216
    }
   ],
   "source": [
    "#now, to combine the list we will use a list comprehension and the zip function\n",
    "\n",
    "exbr_complete = [part1 + part2 for part1, part2 in zip(exbr_part1_list, exbr_part2_list)] \n",
    "\n",
    "exbr_complete[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "source": [
    "#check length once last time\n",
    "\n",
    "len(exbr_complete)"
   ]
  },
  {
   "source": [
    "Past this point, without metadata, I lack the full confidence to place any variable into a specific column. However, every element is still uniquely identifiable, so despite not knowing what they represent, they still can be isolated and made into a clean .csv file as discussed at the begining of this notebook.\n",
    "\n",
    "The next variable is the hour:minute:second variable spoken of prior. It similarly is spread into 2 parts and can therefore be handled in roughly the same manner.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10:20:\n04\n"
     ]
    }
   ],
   "source": [
    "pull_time = re.compile('\\s(\\d\\d:\\d\\d:).*\\s(\\d\\d)\\s')\n",
    "\n",
    "\n",
    "print(pull_time.search(Workingcontent[1])[1])\n",
    "\n",
    "print(pull_time.search(Workingcontent[1])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_part1_list = [pull_time.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "time_part2_list = [pull_time.search(line)[2] for line in Workingcontent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['09:30:', '10:20:', '10:20:', '10:20:', '10:22:', '10:24:', '10:24:', '10:24:', '10:24:', '10:24:']\n['00', '04', '04', '23', '29', '01', '01', '01', '01', '01']\n"
     ]
    }
   ],
   "source": [
    "print(time_part1_list[0:10])\n",
    "\n",
    "print(time_part2_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "913\n913\n"
     ]
    }
   ],
   "source": [
    "print(len(time_part1_list))\n",
    "print(len(time_part2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['09:30:00',\n",
       " '10:20:04',\n",
       " '10:20:04',\n",
       " '10:20:23',\n",
       " '10:22:29',\n",
       " '10:24:01',\n",
       " '10:24:01',\n",
       " '10:24:01',\n",
       " '10:24:01',\n",
       " '10:24:01']"
      ]
     },
     "metadata": {},
     "execution_count": 222
    }
   ],
   "source": [
    "#combine again\n",
    "\n",
    "time_complete = [part1 + part2 for part1, part2 in zip(time_part1_list, time_part2_list)] \n",
    "\n",
    "time_complete[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "913\n"
     ]
    }
   ],
   "source": [
    "print(len(time_complete))"
   ]
  },
  {
   "source": [
    "Next, we have what I believe is the TradeCommtype. It makes up the remaining characters that come right after the first part of the time variable we just pulled out, meaning we need only to slightly modify our current logic to grab it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ContraSSFREX\n"
     ]
    }
   ],
   "source": [
    "pull_TradeCommtype = re.compile('\\s\\d\\d:\\d\\d:(\\S+)')\n",
    "\n",
    "\n",
    "print(pull_TradeCommtype.search(Workingcontent[1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CustSS',\n",
       " 'ContraSSFREX',\n",
       " 'ContraSSFREX',\n",
       " 'ContraSSFREX',\n",
       " 'ContraSSFREX',\n",
       " 'ContraSSFREX',\n",
       " 'ContraSSFREX',\n",
       " 'ContraSSFREX',\n",
       " 'ContraSSFREX',\n",
       " 'ContraSSFREX']"
      ]
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "TradeCommtype_list = [pull_TradeCommtype.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "TradeCommtype_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "len(TradeCommtype_list)"
   ]
  },
  {
   "source": [
    "The next variable Is what I presume to be the account number, as it is usually found in close relation to that category and remains consistant throughout the entire pull. But again, for logic puproses, we still will right a simple regex to grab it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0704\n"
     ]
    }
   ],
   "source": [
    "pull_accountid = re.compile('\\s(\\d\\d\\d\\d)\\s')\n",
    "\n",
    "\n",
    "print(pull_accountid.search(Workingcontent[1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['2756',\n",
       " '0704',\n",
       " '0704',\n",
       " '0704',\n",
       " '0704',\n",
       " '0704',\n",
       " '0704',\n",
       " '0704',\n",
       " '0704',\n",
       " '0704']"
      ]
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "source": [
    "accountid_list = [pull_accountid.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "accountid_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "source": [
    "len(accountid_list)"
   ]
  },
  {
   "source": [
    "The next variable is always perunit, but I am not sure which variable it belongs under. Regardless, it will be pulled in exactly the same matter, only more discreetly to avoid certain difficulty."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PerUnit\n"
     ]
    }
   ],
   "source": [
    "pull_PerUnit = re.compile('\\s(PerUnit)\\s')\n",
    "\n",
    "\n",
    "print(pull_PerUnit.search(Workingcontent[1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['PerUnit',\n",
       " 'PerUnit',\n",
       " 'PerUnit',\n",
       " 'PerUnit',\n",
       " 'PerUnit',\n",
       " 'PerUnit',\n",
       " 'PerUnit',\n",
       " 'PerUnit',\n",
       " 'PerUnit',\n",
       " 'PerUnit']"
      ]
     },
     "metadata": {},
     "execution_count": 231
    }
   ],
   "source": [
    "perUnit_list = [pull_PerUnit.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "perUnit_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 232
    }
   ],
   "source": [
    "len(perUnit_list)"
   ]
  },
  {
   "source": [
    "Term is in the exact same boat as PerUnit, the previous solution is used again but simplified further to keep things fresh."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 233
    }
   ],
   "source": [
    "pull_TERM = re.compile('\\s(TERM)\\s')\n",
    "\n",
    "Term_list = [pull_TERM.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "len(Term_list)\n"
   ]
  },
  {
   "source": [
    "As we begin to finish up, we come across a variable that appears to be a floating point variable isolated by itself that comes at some point after the time variable of the form (0.xxxxxx). Though the implementation gets a bit stale, we remind ourselves that we want the data in this multi-list format incase we get the variable wrong so that they may be swapped easily."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.010000\n"
     ]
    }
   ],
   "source": [
    "pull_float = re.compile('\\s(0\\.[\\d]{6})\\s')\n",
    "\n",
    "\n",
    "print(pull_float.search(Workingcontent[1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['0.010000',\n",
       " '0.010000',\n",
       " '0.010000',\n",
       " '0.010000',\n",
       " '0.010000',\n",
       " '0.010000',\n",
       " '0.010000',\n",
       " '0.010000',\n",
       " '0.010000',\n",
       " '0.010000']"
      ]
     },
     "metadata": {},
     "execution_count": 235
    }
   ],
   "source": [
    "float_list = [pull_float.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "float_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "len(float_list)"
   ]
  },
  {
   "source": [
    "As we come into the last three items, we are met with a variable that conists of simply 5 to 6 capital letters. The regex once again follows the same basic pattern."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AABBC\n"
     ]
    }
   ],
   "source": [
    "pull_Charvar = re.compile('\\s([A-Z]{5,6})\\s')\n",
    "\n",
    "\n",
    "print(pull_Charvar.search(Workingcontent[1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['AABBC',\n",
       " 'AABBC',\n",
       " 'AABBC',\n",
       " 'AABBC',\n",
       " 'AABBC',\n",
       " 'AABBC',\n",
       " 'AABBC',\n",
       " 'AABBC',\n",
       " 'AABBC',\n",
       " 'AABBC']"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "Charvar_list = [pull_Charvar.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "Charvar_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "source": [
    "len(Charvar_list)"
   ]
  },
  {
   "source": [
    "The penultimate data item (ex. 1706149900003#STA=GSCO) is fairly straitforward to grab, as it is the only item to always have an octothorpe (#) in its name. However, its exact nature is perplexing, as every portion of the value seems listed elsewhere, as it seems to be a combination of the first part of what I have called Exbkr, STA=, and then a Ticker Symbol which may or may not match the one already in place. Despite this, data is data, and it will be pulled and isolated as such. It may or may not be on the end of a line as well, so that must be accounted for.\n",
    "\n",
    "It may also be more then one variable, but again, without metadata, I have no way to confidently make a distinction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1706149900003#STA=CERS\n"
     ]
    }
   ],
   "source": [
    "pull_Unknown_Octo = re.compile('\\s(\\S+#\\S+)\\s?')\n",
    "\n",
    "print(pull_Unknown_Octo.search(Workingcontent[1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1706149900003#STA=GSCO',\n",
       " '1706149900003#STA=CERS',\n",
       " '1706149900003#STA=CERS',\n",
       " '1706149900003#STA=CERS',\n",
       " '1706149900003#STA=CERS',\n",
       " '1706149900003#STA=CERS',\n",
       " '1706149900003#STA=CERS',\n",
       " '1706149900003#STA=CERS',\n",
       " '1706149900003#STA=CERS',\n",
       " '1706149900003#STA=CERS']"
      ]
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "Unknown_Octo_list = [pull_Unknown_Octo.search(line)[1] for line in Workingcontent]\n",
    "\n",
    "Unknown_Octo_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 242
    }
   ],
   "source": [
    "len(Unknown_Octo_list)"
   ]
  },
  {
   "source": [
    "The last variable to be grabbed rests on the end, and is an optional 6 digit code similar to part 2 of Exbkr. However, because it only exists optionally, we will have to handle the case where it doesn't show up, so we will need to construct a more complex lambda statement. As for the regex, this code is quick CRTL+F and some minor searching tells us that the code is always either 854300 or 473640, which gives us a pretty discrete solution as far as regex is concerned. It was also ensured that these numbers never appeared anywhere else, so we don't have to worry about false positives.\n",
    "\n",
    "After carefeul consideration, it was decided that these numerica digits were the trailing digits of what is currently called the \"unknown data\" similar to how T-Dats-DatTRadeID had disjointed codes. Furthermore, everything after the # in that unknown portion will be considered as its own unique bit of data and given its own column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\nTrue\n"
     ]
    }
   ],
   "source": [
    "pull_Optional_Code = re.compile('(473640|854300)')\n",
    "\n",
    "\n",
    "#should fail\n",
    "print(bool(pull_Optional_Code.search(Workingcontent[0])))\n",
    "\n",
    "#should succeed\n",
    "print(bool(pull_Optional_Code.search(Workingcontent[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Optional_end_func = lambda line: (pull_Optional_Code.search(line)[1] if bool(pull_Optional_Code.search(line)) == True else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['N/A',\n",
       " '854300',\n",
       " '854300',\n",
       " '854300',\n",
       " '854300',\n",
       " 'N/A',\n",
       " '854300',\n",
       " '854300',\n",
       " '854300',\n",
       " '854300']"
      ]
     },
     "metadata": {},
     "execution_count": 245
    }
   ],
   "source": [
    "\n",
    "Unknown_End_Code = [Optional_end_func(line) for line in Workingcontent]\n",
    "\n",
    "Unknown_End_Code[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "metadata": {},
     "execution_count": 246
    }
   ],
   "source": [
    "len(Unknown_End_Code)"
   ]
  },
  {
   "source": [
    "Now, we need to split up the unkowndata into two columns. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1706149900003#STA=GSCO\n1706149900003\n"
     ]
    }
   ],
   "source": [
    "#test on single case\n",
    "testrow = Unknown_Octo_list[0]\n",
    "\n",
    "print(testrow)\n",
    "\n",
    "print(testrow.partition(\"#\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003',\n",
       " '1706149900003']"
      ]
     },
     "metadata": {},
     "execution_count": 248
    }
   ],
   "source": [
    "#do it for all elements\n",
    "\n",
    "Unknown_ID = [line.partition(\"#\")[0] for line in Unknown_Octo_list]\n",
    "\n",
    "Unknown_ID[0:20]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1706149900003N/A',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003N/A',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003N/A',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300']"
      ]
     },
     "metadata": {},
     "execution_count": 249
    }
   ],
   "source": [
    "#combine with End code.\n",
    "\n",
    "Full_Unknown_Code = [frontcode + backcode for frontcode, backcode in zip(Unknown_ID, Unknown_End_Code)]\n",
    "\n",
    "Full_Unknown_Code[0:20]\n"
   ]
  },
  {
   "source": [
    "This works, but now we have a problem with certain endcodes being missing. Given the pattern, we can surmise what the missing values should be based on the ones surronding it. If it ends in \"3N/A\", it should be 854300, if it ends with \"4N/A\" it should be 473640\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300',\n",
       " '1706149900003854300']"
      ]
     },
     "metadata": {},
     "execution_count": 250
    }
   ],
   "source": [
    "#function to replace\n",
    "\n",
    "Code_fixer = lambda line: line.replace(\"N/A\" , \"854300\") if \"3N/A\" in line else line.replace(\"N/A\", \"473640\")\n",
    "\n",
    "\n",
    "#for all elements\n",
    "\n",
    "Full_Unknown_Code = [Code_fixer(line) for line in Full_Unknown_Code]\n",
    "\n",
    "Full_Unknown_Code[0:20]\n"
   ]
  },
  {
   "source": [
    "Now, we just need to grab the second variable from the original uknown code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['#STA=GSCO',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS',\n",
       " '#STA=CERS']"
      ]
     },
     "metadata": {},
     "execution_count": 251
    }
   ],
   "source": [
    "#use the same partition method\n",
    "\n",
    "\n",
    "Unknown_variable_pound = [line.partition(\"#\")[1] + line.partition(\"#\")[2] for line in Unknown_Octo_list]\n",
    "\n",
    "Unknown_variable_pound[0:20]"
   ]
  },
  {
   "source": [
    "At this point, we have all of the data items pulled out, now it is just a matter of putting them back together in a way that makes sense. Again, as stated above, I do not feel confident in matching certain columns to certain data types, so I tried my best guess and ended up with more columns then were indicated at the start, but if any adjustments are needed it is as simple as recombining the pulled data items to suit whatever the need it.\n",
    "\n",
    "First, we must make the function to combine the lists"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_merger = lambda list1, list2: [part1 + \",\"+ part2 for part1, part2 in zip(list1, list2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CERS,SS,NEW,2756,2.400000,2.340000,2.450000,06/14/17,06/19/17,1706149900003256945,09:30:00,CustSS,2756,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=GSCO',\n",
       " 'CERS,SS,NEW,100,2.3600001842,2.360000,2.370000,06/14/17,06/19/17,1706149900003866087,10:20:04,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,200,2.350000,2.360000,2.370000,06/14/17,06/19/17,1706149900003866096,10:20:04,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.350000,2.350000,2.360000,06/14/17,06/19/17,1706149900003869780,10:20:23,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.350000,2.350000,2.360000,06/14/17,06/19/17,1706149900003892888,10:22:29,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909119,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.3600008543,2.350000,2.360000,06/14/17,06/19/17,1706149900003909128,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909140,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909143,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909148,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909154,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909168,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909177,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.365000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909188,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,300,2.3600008543,2.350000,2.360000,06/14/17,06/19/17,1706149900003909190,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,200,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909193,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909196,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,300,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909219,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909224,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.365000,2.360000,2.370000,06/14/17,06/19/17,1706149900003909672,10:24:04,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS']"
      ]
     },
     "metadata": {},
     "execution_count": 253
    }
   ],
   "source": [
    "#build the list however you want.\n",
    "\n",
    "WorkingOutput = list_merger(Ticker_list,  side_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  cxl_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  qty_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  price_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  bid_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  ask_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  T_DatS_DatTradeID_list_fixed)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  TradeTiSS_list_fixed)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput, exbr_complete)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  time_complete)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  TradeCommtype_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  accountid_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  perUnit_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  Term_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  float_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  Charvar_list)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  Full_Unknown_Code)\n",
    "\n",
    "WorkingOutput = list_merger(WorkingOutput,  Unknown_variable_pound)\n",
    "\n",
    "\n",
    "WorkingOutput[0:20]"
   ]
  },
  {
   "source": [
    "Now, the columns need tobe named. Without metadata or an example output, columns names were taken from a combination of the original names and new names the describe the variable to the best of my understanding. The former were used whenever possible, but due to my suspicion that the data was originally output with certain columns violated the golden rule even when in a clean state, some were parititoned or thrown out.\n",
    "\n",
    "Even as they stand though, the column names do not appear to make a whole lot of sense. However, without the context, perhaps they never will. Whats more important is that the data items are isolated, the names at this point are trivial and can be changed on a whim if nescescary. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Symbol,Side,CxL,Qty,Price,Bid,Ask,StartDate,EndDate,TradeID,Time,Exkbr,TradeCommType,SourceComission,AccountType,OrderID,GTL,TrailerInfo,Clr',\n",
       " 'CERS,SS,NEW,2756,2.400000,2.340000,2.450000,06/14/17,06/19/17,1706149900003256945,09:30:00,CustSS,2756,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=GSCO',\n",
       " 'CERS,SS,NEW,100,2.3600001842,2.360000,2.370000,06/14/17,06/19/17,1706149900003866087,10:20:04,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,200,2.350000,2.360000,2.370000,06/14/17,06/19/17,1706149900003866096,10:20:04,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.350000,2.350000,2.360000,06/14/17,06/19/17,1706149900003869780,10:20:23,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.350000,2.350000,2.360000,06/14/17,06/19/17,1706149900003892888,10:22:29,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909119,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.3600008543,2.350000,2.360000,06/14/17,06/19/17,1706149900003909128,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909140,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909143,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909148,10:24:01,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909154,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909168,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909177,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.365000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909188,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,300,2.3600008543,2.350000,2.360000,06/14/17,06/19/17,1706149900003909190,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,200,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909193,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909196,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,300,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909219,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS',\n",
       " 'CERS,SS,NEW,100,2.360000,2.350000,2.360000,06/14/17,06/19/17,1706149900003909224,10:24:02,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS']"
      ]
     },
     "metadata": {},
     "execution_count": 254
    }
   ],
   "source": [
    "#add add a row to act as columns\n",
    "\n",
    "Columns = \"Symbol,Side,CxL,Qty,Price,Bid,Ask,StartDate,EndDate,TradeID,Time,Exkbr,TradeCommType,SourceComission,AccountType,OrderID,GTL,TrailerInfo,Clr\"\n",
    "\n",
    "WorkingOutput.insert(0, Columns)\n",
    "\n",
    "WorkingOutput[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Symbol,Side,CxL,Qty,Price,Bid,Ask,StartDate,EndDate,TradeID,Time,Exkbr,TradeCommType,SourceComission,AccountType,OrderID,GTL,TrailerInfo,Clr\\nCERS,SS,NEW,2756,2.400000,2.340000,2.450000,06/14/17,06/19/17,1706149900003256945,09:30:00,CustSS,2756,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=GSCO\\nCERS,SS,NEW,100,2.3600001842,2.360000,2.370000,06/14/17,06/19/17,1706149900003866087,10:20:04,ContraSSFREX,0704,PerUnit,TERM,0.010000,AABBC,1706149900003854300,#STA=CERS\\nCERS,SS,NEW,200,2.350000,2.3'"
      ]
     },
     "metadata": {},
     "execution_count": 255
    }
   ],
   "source": [
    "#now, make it all one line\n",
    "\n",
    "WorkingOutput_in_one_line = \"\\n\".join(WorkingOutput)\n",
    "\n",
    "WorkingOutput_in_one_line[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wite to .csv\n",
    "\n",
    "with open('Final_output_adjusted2.csv','w') as outfile:\n",
    "    outfile.write(WorkingOutput_in_one_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}